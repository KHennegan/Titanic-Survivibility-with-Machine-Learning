{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH.1 MACHINE LEARNING BASICS: Why Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple: Python makes it easy.\n",
    "\n",
    "- Popular = good documentation, Stack Overflow activity, myriad of tutorials\n",
    "- Extensive and useful libraries\n",
    "- Easy to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmch\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "model = LogisticRegression().fit(iris['data'], iris['target'])\n",
    "model.predict(iris['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH.2 Exploratory Data Analysis and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis: WHY?\n",
    "- Helps us understand the shape of data\n",
    "- learn useful features\n",
    "- informs how to clean the data\n",
    "\n",
    "WHAT are EDA processes?\n",
    "- Analyze counts or distrubtions of variables\n",
    "- determine data type for each feature\n",
    "- Identiffy and deal with missing data or duplicated data\n",
    "- Find Correlations\n",
    "\n",
    "EDA helps us build the foundation of our model and a clearer picture of how to build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning: WHY?\n",
    "- Shape data for the model\n",
    "- Remove irrelavent Data\n",
    "- Adjust features for the model\n",
    "WHAT are Data Cleaning proceses?\n",
    "- Anonymize! Important to respect data privacy!\n",
    "- Encode catagorized variables\n",
    "- Fill in missing data\n",
    "- Prune or scale data as needed\n",
    "\n",
    "Data cleaning ensures we have good input for the model to work with! Otherwise, results will not be as accurrate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH.3 SPLITTING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How we ensure the model is learning and not just memorizing\n",
    "\n",
    "TYPES OF DATA:\n",
    "- Training Dataset: data used in training\n",
    "- Validation Dataset: data used to select best model\n",
    "    - Must be completel unseen. Not learned\n",
    "- Test Dataset: data used to provide an unbiaed evaluation of model\n",
    "\n",
    "FULL DATASET = 60% Training Data + 20% Validation Data + 20% Testing Data\n",
    "\n",
    "Splitting Data Avoids:\n",
    "- Over/Under fitting data\n",
    "- Inacurate repersentation of the model's generalization\n",
    "\n",
    "Holdout Test Set: Sample data not used for fitting model but to evaluate ability to generaliz unseen data\n",
    "\n",
    "K-Folds Gross Validation: Data divided into k subsets and holdout is repeated k times. Each time one subset is used as test set the other k-1 subsets are used in training\n",
    "\n",
    "Evaluation Frameworks:\n",
    "1. Evaluation Metrics- How are we gaging the accuracy?\n",
    "2. Process- How do we mitigate under/over fitting\n",
    "     *Our example metrics, a classification problem...\n",
    "         Accuracy = #predicted correct/total examples\n",
    "         Percision = #predicted suvived and did/total #predicted survived\n",
    "         Recall = #predicted survived and did/total that actually survived*\n",
    "\n",
    "Bias: Algorithms tendency to consistently learn the wrong thing by not taking in account all info in data\n",
    "\n",
    "High Bias: result of algorithm missing relevent relations between features and outputs\n",
    "\n",
    "Variance: Sensitivity to small fluctuations in the data\n",
    "\n",
    "High Variance: Algorithm fitting to random noise in training data\n",
    "\n",
    "Underfitting: when the algorithm cannot deetermine the underlying patterns in the data\n",
    "\n",
    "OVeritting: when the algorithm fits too close & memorizes the examples instead of the patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH.4 OPTIMIZING MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary Ways to Tune for Optimal Complexity:\n",
    "    1. Hperparameter tuning: choosing a set of hyper parameters for fitting an algorithm\n",
    "    2. Regularization: reuces overfitting by discouraging overly complex models\n",
    "   \n",
    "- Hyperparameters are determined not by training data but structure the models' fitting\n",
    "- Ridge or Lasso Regression: adding a penalty to the loss function to constrain coefficients\n",
    "- Dopout: Some nodes are ignored during training forcing other nodes to take more/less weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
